{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Demo.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "7AhBEBMnNqoz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3cc480bc-3aab-4672-c771-ab7cf4124b06"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKsJEGyjOAxE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "a0332a9c-190c-4a37-bc44-0e3bbddd0088"
      },
      "source": [
        "import os\n",
        "os.chdir('/content/drive/My Drive/sentence_selection')\n",
        "!head train.tsv # display some training examples, format: sentence1,tab,sentence2,tab,label"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "So what a ya do with the companies once you buy'm?\tI sell them.\t1\n",
            "Yeah, my grammy.\tSee, I'm new.\t0\n",
            "Jesus.\tHer name is Kristen. She disappeared a couple of months ago.\t1\n",
            "How do you *like* this! You don't suppose that ranger met up with some kids--and took 'em for a hike!\tThat--or he's out blazing trails. He'll show up.\t1\n",
            "Oh, this is a dangerous game you're playing, Johns.\tMay've noticed chains don't work on this guy. Only way we're truly safe is if he believes he's goin' free. But if he <u>stops</u> believin' --\t1\n",
            "I could hide you, Tommy.\tI know you would, Ma. But I ain't gonna let you. You hide somebody that's kilt a man an'... an' you'd be in trouble too.\t1\n",
            "Honey.\tHow are you?\t1\n",
            "It's like the last time. He sent me a present before he --\tWhatya mean?  We're going!  Tran's gonna do her right there unless--\t0\n",
            "I'm sorry.  I can't.  I....\tWhatta you mean? Me?\t0\n",
            "Oh, that's right. That you really like those New York girls.\tWell, no... not just, not only.\t1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9GI4_H8pOHsh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install pytorch-transformers -q # install python library for pretrained BERT (and other similar) models"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfHEZrKAOJ2J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import random\n",
        "import time\n",
        "import math\n",
        "from pytorch_transformers import DistilBertModel as BertModel\n",
        "from pytorch_transformers import DistilBertTokenizer as BertTokenizer\n",
        "random.seed(1234)\n",
        "torch.manual_seed(1234)\n",
        "torch.cuda.set_device(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_evDAlZOKV3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SPL_SYMS = ['<PAD>','<BOS>', '<EOS>', '<UNK>']\n",
        "\n",
        "\n",
        "class STSCorpus(object):\n",
        "  def __init__(self,\n",
        "              file,\n",
        "              vocab=None,\n",
        "              cuda=False,\n",
        "              batch_size=1, bert_format=0):\n",
        "    self.bert_format = bert_format\n",
        "    if self.bert_format == 0:\n",
        "      self.bert_tokenizer = None\n",
        "      self.max_vocab = 64000\n",
        "    else:\n",
        "      self.bert_tokenizer = BertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "      self.max_vocab = self.bert_tokenizer.vocab_size\n",
        "    self.max_size = 0\n",
        "    self.batch_size = batch_size\n",
        "    self.vocab = self.make_vocab(file, vocab)\n",
        "    self.idx2vocab = self.make_idx2vocab(self.vocab)\n",
        "    self.data = self.numberize(file, self.vocab, cuda)\n",
        "    self.batch_data = self.batchify()\n",
        "    self.data_size = len(self.batch_data)\n",
        "\n",
        "  def batchify(self,):\n",
        "    self.batch_data = []\n",
        "    curr_batch = []\n",
        "    max_x1, max_x2 = 0, 0\n",
        "    for x1, x2, y in self.data:\n",
        "      if len(curr_batch) < self.batch_size:\n",
        "        curr_batch.append((x1, x2, y))\n",
        "        max_x1 = max(max_x1, x1.shape[1])\n",
        "        if self.bert_format == 0:\n",
        "          max_x2 = max(max_x2, x2.shape[1]) \n",
        "      else:\n",
        "        \n",
        "        _x1, _x2, _y = zip(*curr_batch)\n",
        "        \n",
        "        \n",
        "        if self.bert_format == 0:\n",
        "          _x1 = [torch.cat((torch.zeros(1, max_x1 - i.shape[1]).type_as(i), i), dim=1) for i in _x1]\n",
        "          batch_x1 = torch.cat(_x1, dim=0)\n",
        "          _x2 = [torch.cat((torch.zeros(1, max_x2 - i.shape[1]).type_as(i), i), dim=1) for i in _x2]\n",
        "          batch_x2 = torch.cat(_x2, dim=0) if _x2[0] is not None else None\n",
        "        else:\n",
        "          _x1 = [torch.cat((i, torch.zeros(1, max_x1 - i.shape[1]).type_as(i)), dim=1) for i in _x1]\n",
        "          batch_x1 = torch.cat(_x1, dim=0)\n",
        "          batch_x2 = None\n",
        "        batch_y = torch.cat(_y, dim=0)\n",
        "        self.batch_data.append((batch_x1, batch_x2, batch_y))\n",
        "        curr_batch = []\n",
        "        max_x1, max_x2 = 0, 0\n",
        "    # remaining items in curr_batch\n",
        "    if len(curr_batch) > 0:\n",
        "      print(len(self.batch_data),  max_x1, max_x2)\n",
        "      _x1, _x2, _y = zip(*curr_batch)\n",
        "      \n",
        "      \n",
        "      if self.bert_format == 0:\n",
        "        _x1 = [torch.cat((torch.zeros(1, max_x1 - i.shape[1]).type_as(i), i), dim=1) for i in _x1]\n",
        "        batch_x1 = torch.cat(_x1, dim=0)\n",
        "        _x2 = [torch.cat((torch.zeros(1, max_x2 - i.shape[1]).type_as(i), i), dim=1) for i in _x2]\n",
        "        batch_x2 = torch.cat(_x2, dim=0) if _x2[0] is not None else None\n",
        "      else:\n",
        "        _x1 = [torch.cat((i, torch.zeros(1, max_x1 - i.shape[1]).type_as(i)), dim=1) for i in _x1]\n",
        "        batch_x1 = torch.cat(_x1, dim=0)\n",
        "        batch_x2 = None\n",
        "      batch_y = torch.cat(_y, dim=0)\n",
        "      self.batch_data.append((batch_x1, batch_x2, batch_y))\n",
        "    return self.batch_data\n",
        "\n",
        "  def numberize(self, txt, vocab, cuda=False):\n",
        "    data = []\n",
        "    max_size = 0\n",
        "    with open(txt, 'r', encoding='utf8') as corpus:\n",
        "      for l in corpus:\n",
        "        #print(l)\n",
        "        l1, l2, y = l.split('\\t')[-3:]\n",
        "        y = torch.Tensor([[float(y)]]).float()\n",
        "        if self.bert_format == 0:\n",
        "          d1 = [vocab['<BOS>']] + [vocab.get(t, vocab['<UNK>']) for t in l1.strip().split()] + [vocab['<EOS>']]\n",
        "          d1 = torch.Tensor(d1).long()\n",
        "          d1 = d1.unsqueeze(0) # shape = (1, N)\n",
        "          d2 = [vocab['<BOS>']] + [vocab.get(t, vocab['<UNK>']) for t in l2.strip().split()] + [vocab['<EOS>']]\n",
        "          d2 = torch.Tensor(d2).long()\n",
        "          d2 = d2.unsqueeze(0) # shape = (1, N)\n",
        "          max_size = max(d1.shape[1], d2.shape[1], max_size)\n",
        "          if cuda:\n",
        "            d1 = d1.cuda()\n",
        "            d2 = d2.cuda()\n",
        "            y = y.cuda()\n",
        "        elif self.bert_format == 1:\n",
        "          _d1 = torch.Tensor(self.bert_tokenizer.encode(\"[CLS] \" + l1 + \" [SEP]\")).long()\n",
        "          _d2 = torch.Tensor(self.bert_tokenizer.encode(\" \" + l2 + \" [SEP]\")).long()\n",
        "          d = torch.cat([_d1, _d2], dim=0).unsqueeze(0)\n",
        "          max_size = max(d.shape[1], max_size)\n",
        "          if cuda:\n",
        "            d1 = d.cuda()\n",
        "            d2 = None\n",
        "            y = y.cuda()\n",
        "        else:\n",
        "          pass\n",
        "        data.append((d1, d2, y))\n",
        "    self.max_size = max_size\n",
        "    return data\n",
        "\n",
        "  def make_idx2vocab(self, vocab):\n",
        "    if vocab is not None:\n",
        "      idx2vocab = {v: k for k, v in vocab.items()}\n",
        "      return idx2vocab\n",
        "    else:\n",
        "      return None\n",
        "\n",
        "  def make_vocab(self, txt, vocab):\n",
        "    if vocab is None and txt is not None:\n",
        "      vc = {}\n",
        "      for line in open(txt, 'r', encoding='utf-8').readlines():\n",
        "        #print(\"line:\" + line)\n",
        "        x1, x2, y = line.strip().split('\\t')[-3:]\n",
        "        for w in x1.split() + x2.split():\n",
        "          vc[w] = vc.get(w, 0) + 1\n",
        "      cv = sorted([(c, w) for w, c in vc.items()], reverse=True)\n",
        "      cv = cv[:self.max_vocab]\n",
        "      _, v = zip(*cv)\n",
        "      v = SPL_SYMS + list(v)\n",
        "      vocab = {w: idx for idx, w in enumerate(v)}\n",
        "      return vocab\n",
        "    else:\n",
        "      return vocab\n",
        "\n",
        "  def get(self, idx):\n",
        "    return self.batch_data[idx]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rz3-cINGUnm_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Classifier(torch.nn.Module):\n",
        "    def __init__(self,\n",
        "                 vocab_size,\n",
        "                 embedding_size,\n",
        "                 hidden_size,\n",
        "                 num_layers=1,\n",
        "                 dropout=0.1,\n",
        "                 max_grad_norm=5.0):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.embedding_size = embedding_size\n",
        "        self.max_grad_norm = max_grad_norm\n",
        "        self.dropout_layer = torch.nn.Dropout(p = dropout)\n",
        "        \n",
        "        if max(vocab_size,embedding_size ,hidden_size,num_layers) > 0:\n",
        "          self.embedding_layer = torch.nn.Embedding(num_embeddings = vocab_size, embedding_dim = self.embedding_size)\n",
        "          \n",
        "          self.uni_RNN_LSTM_layer = torch.nn.LSTM(input_size = self.embedding_size, hidden_size = self.hidden_size, num_layers=self.num_layers,  dropout = dropout, batch_first= True)\n",
        "          self.output = torch.nn.Linear(in_features=self.hidden_size * 2, out_features = 1)\n",
        "          \n",
        "\n",
        "\n",
        "          self.optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, self.parameters()))\n",
        "        else:\n",
        "          pass\n",
        "        self.loss = torch.nn.BCELoss(reduction='mean')\n",
        "          \n",
        "\n",
        "    def predict(self, x1, x2):\n",
        "        \"\"\" Generates a prediction and probability for each input instance\n",
        "        Args:\n",
        "            x1: sequence of input tokens for the first sentence\n",
        "            x2: sequence of input tokens for the second sentence\n",
        "        Returns:\n",
        "            out: sequence of output predictions (probabilities) for each instance\n",
        "            pred: the discrete prediction from the output probabilities\n",
        "        \"\"\"\n",
        "        batch_size, seq_len = x1.shape\n",
        "        batch_size2, seq_len2 = x2.shape\n",
        "        assert batch_size == batch_size2\n",
        "        \n",
        "        emb_x1 = self.dropout_layer(self.embedding_layer(x1))\n",
        "        \n",
        "        emb_x2 = self.dropout_layer(self.embedding_layer(x2))\n",
        "\n",
        "        h, c = (torch.zeros(self.num_layers, batch_size, self.hidden_size).cuda(),\n",
        "                torch.zeros(self.num_layers, batch_size, self.hidden_size).cuda())\n",
        "        x1_out, (x1_hidden, x1_cell) = self.uni_RNN_LSTM_layer(emb_x1, (h, c))\n",
        "        x2_out, (x2_hidden, x2_cell) = self.uni_RNN_LSTM_layer(emb_x2, (h, c))\n",
        "        final_hidden = torch.cat((x1_out[:,-1,:].squeeze(1), x2_out[:,-1,:].squeeze(1)), -1)\n",
        "        final_hidden = self.dropout_layer(final_hidden)\n",
        "        out = torch.sigmoid(self.output(final_hidden))\n",
        "\n",
        "        pred = out.clone().detach()\n",
        "        pred[pred >= 0.5] = 1\n",
        "        pred[pred < 0.5] = 0\n",
        "        return out, pred\n",
        "\n",
        "    def forward(self, x1, x2, y):\n",
        "        out, pred = self.predict(x1,x2)\n",
        "        loss = self.loss(out, y)\n",
        "\n",
        "        assert pred.shape == y.shape\n",
        "        acc = (pred == y).sum().item() / y.numel()\n",
        "        return loss, acc\n",
        "\n",
        "    def train_step(self, x1, x2, y):\n",
        "        self.optimizer.zero_grad()\n",
        "        _loss, acc = self(x1, x2, y) # calls self.forward(x, y)\n",
        "        _loss.backward()\n",
        "        grad_norm = torch.nn.utils.clip_grad_norm_(filter(lambda p: p.requires_grad, self.parameters()),\n",
        "                                                   self.max_grad_norm)\n",
        "\n",
        "        if math.isnan(grad_norm):\n",
        "            print('skipping update grad_norm is nan!')\n",
        "        else:\n",
        "            self.optimizer.step()\n",
        "        loss = _loss.item()\n",
        "        return loss, acc\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_Q-zmMyUoL2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BERTClassifier(Classifier):\n",
        "    def __init__(self,\n",
        "                 dropout=0.1,\n",
        "                 max_grad_norm=5.0):\n",
        "        super().__init__(0, 0, 0, 0, dropout, max_grad_norm)\n",
        "        self.output = torch.nn.Linear(768, 1)\n",
        "        weight = torch.nn.init.normal_(torch.zeros(1,768), mean = 0, std = 0.05)\n",
        "        self.output.weight = torch.nn.Parameter(weight)\n",
        "        self.bert_model = BertModel.from_pretrained('distilbert-base-uncased')\n",
        "        self.optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, self.parameters()), lr=1e-5)\n",
        "\n",
        "    def predict(self, x1, x2=None):\n",
        "        assert x2 is None\n",
        "        x2 = self.bert_model(x1)\n",
        "        out = torch.sigmoid(self.output(x2[0][:,-1,:].squeeze(1)))\n",
        "\n",
        "        pred = out.clone().detach()\n",
        "        pred[pred >= 0.5] = 1\n",
        "        pred[pred < 0.5] = 0\n",
        "        return out, pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1qNPHgOON3h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def find_best_pair(model, test_corpus, input_sentence):\n",
        "      #print(\"The best answer is:\")\n",
        "      biggest_prob = 0\n",
        "      answer_index = 0\n",
        "      model.eval()\n",
        "      for test_i in range(test_corpus.data_size):\n",
        "        x1, x2, y = test_corpus.get(test_i)\n",
        "        out, pred = model.predict(x1, x2)\n",
        "        if out.item() > biggest_prob:\n",
        "          biggest_prob = out.item()\n",
        "          answer_index = test_i\n",
        "      return answer_index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1M-n1vPsWVjd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "08493272-f4de-4762-f424-025829be9a49"
      },
      "source": [
        "  train_corpus = STSCorpus(file='train.tsv',\n",
        "                            cuda=True,\n",
        "                            batch_size=32, bert_format=1)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1634 45 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NofKTfBJOkgh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bert_model = torch.load('2020-01-16-06:28:57fine-tuned-bert-model.pth.tar')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nf4Mv6PPOlSZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "candidate_file = open('candidate.txt', 'r').readlines()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pmAFtMs_Sw1V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_answer(input):\n",
        "  tmp_file = open('tmp.tsv', 'w')\n",
        "  for line in candidate_file:\n",
        "    x2 = line.strip()\n",
        "    tmp_file.write(input + '\\t' + x2 + '\\t' + '1' + '\\n')\n",
        "  tmp_file.close()\n",
        "  tmp_corpus = STSCorpus(file='tmp.tsv',vocab=train_corpus.vocab,\n",
        "                        cuda=True,\n",
        "                        batch_size=1,bert_format=1)\n",
        "  \n",
        "  answer_id = find_best_pair(bert_model, tmp_corpus, input)\n",
        "  #print(answer_id)\n",
        "  answer = candidate_file[answer_id].strip()\n",
        "  return answer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrhXvHrDT0bI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "15932ab9-c1a4-46d1-de5c-54ff8535709e"
      },
      "source": [
        "  input = 'How are you?'\n",
        "  print(get_answer(input))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "In that case, fine.  Want to see my magic trick?\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}